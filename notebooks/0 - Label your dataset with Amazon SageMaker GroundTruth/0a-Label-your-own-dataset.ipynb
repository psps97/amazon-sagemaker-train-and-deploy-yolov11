{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4379a998-cad7-40fb-99bb-5a2948f7009d",
   "metadata": {},
   "source": [
    "# Label your dataset with Amazon SageMaker Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47a5f73-c8c5-4ce0-bb5d-b996066e47c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy\n",
    "import os\n",
    "import sagemaker\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "s3_resource = boto3.resource('s3')\n",
    "sm_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c33f6-78f7-42da-affe-b34bb34845e0",
   "metadata": {},
   "source": [
    "#### Create a labeling job in Amazon SageMaker Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1452c83-5ce2-4b50-9d11-fc4b8861ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### If you don't have a suitable sample dataset, use the sample data below. \n",
    "#### Copy the sample file in S3 to your own s3 bucket as a sample file for bounding box labelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c3076a-fcf8-4404-b9ad-c24d3fe4b913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://sagemaker-sample-files/datasets/image/caltech-101/inference/image_0007.jpeg to s3://sagemaker-us-west-2-986221661979/raw_images/image_0007.jpeg\n",
      "copy: s3://sagemaker-sample-files/datasets/image/caltech-101/inference/image_0016.jpeg to s3://sagemaker-us-west-2-986221661979/raw_images/image_0016.jpeg\n",
      "copy: s3://sagemaker-sample-files/datasets/image/caltech-101/inference/image_0260.jpeg to s3://sagemaker-us-west-2-986221661979/raw_images/image_0260.jpeg\n",
      "copy: s3://sagemaker-sample-files/datasets/image/caltech-101/inference/image_0123.jpeg to s3://sagemaker-us-west-2-986221661979/raw_images/image_0123.jpeg\n",
      "copy: s3://sagemaker-sample-files/datasets/image/caltech-101/inference/image_0068.jpeg to s3://sagemaker-us-west-2-986221661979/raw_images/image_0068.jpeg\n",
      "copy: s3://sagemaker-sample-files/datasets/image/caltech-101/inference/image_0151.jpeg to s3://sagemaker-us-west-2-986221661979/raw_images/image_0151.jpeg\n",
      "copy: s3://sagemaker-sample-files/datasets/image/caltech-101/inference/image_0059.jpeg to s3://sagemaker-us-west-2-986221661979/raw_images/image_0059.jpeg\n",
      "copy: s3://sagemaker-sample-files/datasets/image/caltech-101/inference/image_0019.jpeg to s3://sagemaker-us-west-2-986221661979/raw_images/image_0019.jpeg\n",
      "copy: s3://sagemaker-sample-files/datasets/image/caltech-101/inference/image_0061.jpeg to s3://sagemaker-us-west-2-986221661979/raw_images/image_0061.jpeg\n",
      "copy: s3://sagemaker-sample-files/datasets/image/caltech-101/inference/image_0271.jpeg to s3://sagemaker-us-west-2-986221661979/raw_images/image_0271.jpeg\n",
      "Copy and paste the below link into a web browser to confirm the ten images were successfully uploaded to your bucket:\n",
      "https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-986221661979/images/\n",
      "\n",
      "When prompted by Sagemaker to enter the S3 location for input datasets, you can paste in the below S3 URL\n",
      "s3://sagemaker-us-west-2-986221661979/images/\n",
      "\n",
      "When prompted by Sagemaker to Specify a new location, you can paste in the below S3 URL\n",
      "s3://sagemaker-us-west-2-986221661979/labeled-data/\n"
     ]
    }
   ],
   "source": [
    "bucket = sm_session.default_bucket()\n",
    "\n",
    "!aws s3 sync s3://sagemaker-sample-files/datasets/image/caltech-101/inference/ s3://{bucket}/raw_images/\n",
    "\n",
    "print('Copy and paste the below link into a web browser to confirm the ten images were successfully uploaded to your bucket:')\n",
    "print(f'https://s3.console.aws.amazon.com/s3/buckets/{bucket}/images/')\n",
    "\n",
    "print('\\nWhen prompted by Sagemaker to enter the S3 location for input datasets, you can paste in the below S3 URL')\n",
    "\n",
    "print(f's3://{bucket}/images/')\n",
    "\n",
    "print('\\nWhen prompted by Sagemaker to Specify a new location, you can paste in the below S3 URL')\n",
    "\n",
    "print(f's3://{bucket}/labeled-data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7892e313-d4da-4487-a3b7-c7f28a1a0935",
   "metadata": {},
   "source": [
    "To create your custom model on YOLOv11 you are going to need to label your custom dataset. To label an object detection dataset you may use Amazon SageMaker Ground Truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdda986-63f2-4514-b6eb-57f935771daa",
   "metadata": {},
   "source": [
    "| ⚠️ WARNING: If you have already labeled an object detection dataset with Amazon SageMaker Ground Truth you can skip to the \"**Get Job Details**\" |\n",
    "| -- |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5523582-1a38-47cc-85d4-9c450ccedb87",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create a Labeling Workforce\n",
    "\n",
    "Follow the steps in the SageMaker Ground Truth documentation here: https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-create-private-console.html#create-workforce-labeling-job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7cd63-e965-44d6-9f5b-0be8e7b9e8ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Create your bounding box labeling job\n",
    "\n",
    "Follow the steps in the SageMaker Ground Truth documentation here: https://docs.aws.amazon.com/sagemaker/latest/dg/sms-create-labeling-job-console.html\n",
    "\n",
    "If using the AWS Console, you should create a labeling job with the following options:\n",
    "\n",
    "1. Job name: Set any unique name for the job name, for example \"Object-Detection-bd\".\n",
    "2. Leave the \"I want to specify a label attribute...\" option un-checked.\n",
    "3. Input data setup: Pick \"Automated data setup\".\n",
    "4. Input dataset location: Copy and paste the location of the single folder with your images in S3. Example: \"s3://mybucket/raw_images\".\n",
    "5. Output dataset location: Choose \"Same location as input dataset\".\n",
    "6. Data type: Choose \"Image\".\n",
    "7. IAM Role: Create a new role and give access to the S3 bucket where your images are located, or any S3 bucket.\n",
    "8. Now hit \"Complete data setup\" and wait for it to be ready.\n",
    "9. Task category: Choose \"Image\" and select \"Bounding box\", then hit \"Next\".\n",
    "10. Worker types: Select \"Private\" and choose your team for the \"Private teams\" option.\n",
    "11. For the Bounding box labeling tool: Enter a description and instructions, and for the \"Labels\" section add the relevant labels for your job. ex(\"airplane\", \"car\", \"ferry\", \"helicopter\", \"motorbike\")\n",
    "12. Finally choose \"Create\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa584d-64c4-4e67-a576-59921a6c0f75",
   "metadata": {},
   "source": [
    "### Get Job Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1703a57-9b6a-47b0-96e2-74eba601142a",
   "metadata": {},
   "source": [
    "Once you have finished labeling your images, let's retrieve the information we need to create our dataset in the format YOLOv11 expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807670d9-3cee-47d5-9f30-0b74735f3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth_job_name = \"Object-Detection-bd\" ### <-- Replace with the name you used for your labeling job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "810be406-7cc7-4098-b2c7-80eecd167523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status:  Completed\n",
      "Manifest Uri:  s3://sagemaker-us-west-2-986221661979/raw_images/Object-Detection-bd/manifests/output/output.manifest\n",
      "Labels Uri:  s3://sagemaker-us-west-2-986221661979/raw_images/Object-Detection-bd/annotation-tool/data.json\n"
     ]
    }
   ],
   "source": [
    "response = sm_client.describe_labeling_job(\n",
    "    LabelingJobName=groundtruth_job_name\n",
    ")\n",
    "\n",
    "labelingJobStatus = response[\"LabelingJobStatus\"]\n",
    "manifestUri = response[\"LabelingJobOutput\"][\"OutputDatasetS3Uri\"]\n",
    "labelsListUri = response[\"LabelCategoryConfigS3Uri\"]\n",
    "\n",
    "print(\"Job Status: \",labelingJobStatus)\n",
    "print(\"Manifest Uri: \", manifestUri)\n",
    "print(\"Labels Uri: \", labelsListUri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a30a9-32ec-4100-87f9-d0029ecea96e",
   "metadata": {},
   "source": [
    "### Get labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a93a9c8-4528-46f2-8b7a-85c7bb089134",
   "metadata": {},
   "source": [
    "We need to retrieve the labels from the training job which are located in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acf2cf92-96b0-4707-9799-8aafe5dca589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_s3_path(s3_path):\n",
    "    path_parts=s3_path.replace(\"s3://\",\"\").split(\"/\")\n",
    "    bucket=path_parts.pop(0)\n",
    "    key=\"/\".join(path_parts)\n",
    "    return bucket, key\n",
    "\n",
    "def get_labels_list(labels_uri):\n",
    "    labels = []\n",
    "    bucket, key = split_s3_path(labels_uri)\n",
    "    s3_resource.meta.client.download_file(bucket, key, 'labels.json')\n",
    "    with open('labels.json') as f:\n",
    "        data = json.load(f)\n",
    "    for label in data[\"labels\"]:\n",
    "        labels.append(label[\"label\"])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eacbf363-06fe-460f-b559-be889ac59078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  ['Car', 'Airplane', 'Ferry', 'Helicopter', 'Moterbike']\n"
     ]
    }
   ],
   "source": [
    "labels = get_labels_list(labelsListUri)\n",
    "print(\"Labels: \",labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ef3ce-40d2-40cf-83c3-7412f37503ba",
   "metadata": {},
   "source": [
    "### Get manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f35b0b3-edbe-4e11-b8d7-1c6970581020",
   "metadata": {},
   "source": [
    "We need to retrieve the labeled manifest file from the training job which is located in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb5c5536-cbf7-4f3d-9e75-4d423f05a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_manifest_file(manifest_uri):\n",
    "    bucket, key = split_s3_path(manifest_uri)\n",
    "    s3_resource.meta.client.download_file(bucket, key, 'output.manifest')\n",
    "    return \"output.manifest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "285684f2-26b9-4d03-8e62-76312459055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = get_manifest_file(manifestUri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f337cb0-f48d-4395-8928-0ddcf8a3ec8b",
   "metadata": {},
   "source": [
    "### Split manifest into training and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8e4752-6c18-4fc4-986f-6e63f4532875",
   "metadata": {},
   "source": [
    "Now we have our manifest, let's split our data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "860b8bc9-0e41-4d49-ac49-e3d223704170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The manifest contains 10 annotations.\n",
      "8 will be used for training.\n",
      "2 will be used for validation.\n"
     ]
    }
   ],
   "source": [
    "with open(manifest) as file:\n",
    "    lines = file.readlines()\n",
    "    data = numpy.array(lines)\n",
    "    train_data, validation_data = train_test_split(data, test_size=0.2)\n",
    "    \n",
    "print(\"The manifest contains {} annotations.\".format(len(data)))\n",
    "print(\"{} will be used for training.\".format(len(train_data)))\n",
    "print(\"{} will be used for validation.\".format(len(validation_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a0779c-6aa5-4037-8790-7653f90fe4de",
   "metadata": {},
   "source": [
    "### Create YOLOv11 Training and Validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89948cf6-8e54-4ecb-8b4a-3d6c29dd19ae",
   "metadata": {},
   "source": [
    "Lets download the images and create the annotation files in YOLOv11 expected format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a63680b8-e8c9-4fbd-8944-403fb3479fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [\"dataset/images/train\", \n",
    "        \"dataset/labels/train\",\n",
    "        \"dataset/images/validation\",\n",
    "        \"dataset/labels/validation\"]\n",
    "\n",
    "for directory in dirs:\n",
    "    !mkdir -p {directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05595658-d4fc-4fb2-8143-39167fb600d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_to_yolo(dataset, dataset_category):\n",
    "    print(\"Downloading images and creating labels for the {} dataset\".format(dataset_category))\n",
    "    for line in dataset:\n",
    "        line = json.loads(line)\n",
    "        \n",
    "        # Variables\n",
    "        object_s3_uri = line[\"source-ref\"]\n",
    "        bucket, key = split_s3_path(object_s3_uri)\n",
    "        image_filename = object_s3_uri.split(\"/\")[-1]\n",
    "        txt_filename = '.'.join(image_filename.split(\".\")[:-1]) + \".txt\"\n",
    "        txt_path = \"dataset/labels/{}/{}\".format(dataset_category, txt_filename)\n",
    "        \n",
    "        # Download image\n",
    "        s3_resource.meta.client.download_file(bucket, key, \"dataset/images/{}/{}\".format(dataset_category,image_filename))\n",
    "        \n",
    "        # Create txt with annotations\n",
    "        with open(txt_path, 'w') as target:\n",
    "            for annotation in line[groundtruth_job_name][\"annotations\"]:\n",
    "                class_id = annotation[\"class_id\"]\n",
    "                center_x = (annotation[\"left\"] + (annotation[\"width\"]/2)) / line[groundtruth_job_name][\"image_size\"][0][\"width\"]\n",
    "                center_y = (annotation[\"top\"] + (annotation[\"height\"]/2)) / line[groundtruth_job_name][\"image_size\"][0][\"height\"]\n",
    "                w = annotation[\"width\"] / line[groundtruth_job_name][\"image_size\"][0][\"width\"]\n",
    "                h = annotation[\"height\"] / line[groundtruth_job_name][\"image_size\"][0][\"height\"]\n",
    "                data = \"{} {} {} {} {}\\n\".format(class_id, center_x, center_y, w, h)\n",
    "                target.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b802e890-1622-43a4-86e2-e7276a816382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading images and creating labels for the train dataset\n",
      "Downloading images and creating labels for the validation dataset\n"
     ]
    }
   ],
   "source": [
    "ground_truth_to_yolo(train_data, \"train\")\n",
    "ground_truth_to_yolo(validation_data, \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a247340-635a-4fa1-b5c1-d884ea204091",
   "metadata": {},
   "source": [
    "### Validate the number of downloaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "409f2780-b839-4906-8426-d0bcd6acd2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 elements in dataset/images/train\n",
      "There are 8 elements in dataset/labels/train\n",
      "There are 2 elements in dataset/images/validation\n",
      "There are 2 elements in dataset/labels/validation\n"
     ]
    }
   ],
   "source": [
    "def count_files(dirs):\n",
    "    for directory in dirs:\n",
    "        number = len([1 for x in list(os.scandir(directory)) if x.is_file()])\n",
    "        print(\"There are {} elements in {}\".format(number, directory))\n",
    "\n",
    "count_files(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c8037a-ac58-4b96-add5-34ba671a3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Show images with bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7257e0ed-27a5-4443-9628-d291bacf9aa3",
   "metadata": {},
   "source": [
    "### Upload to S3 the labeled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd6a607-786f-4098-9162-c532e94fe6a3",
   "metadata": {},
   "source": [
    "Let's upload our dataset to S3, this will be used for the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a435d86-44be-4908-aef7-0f80a15d3834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset located in:  s3://sagemaker-us-west-2-986221661979/yolov11dataset\n"
     ]
    }
   ],
   "source": [
    "bucket = sm_session.default_bucket()\n",
    "#bucket = \"\" #Use this option if you want to use a specific S3 bucket\n",
    "dataset_s3_uri = sm_session.upload_data(\"dataset\", bucket, \"yolov11dataset\")\n",
    "print(\"Dataset located in: \",dataset_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548b938-525f-450d-9382-9ce9595fcaa4",
   "metadata": {},
   "source": [
    "You have labeled your own custom dataset with Amazon SageMaker Ground Truth and split it a training and validation dataset in YOLOv11 expected format. For the next modules you will be able to use this dataset to train and deploy a custom YOLOv5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944291de-7144-4585-8662-b0592ea2c98a",
   "metadata": {},
   "source": [
    "| ⚠️ WARNING: These are the details you will need to train your models based on the labeling job you completed. |\n",
    "| -- |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7730b90-2fd6-4cdf-ba90-551cc28424bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset S3 location:  s3://sagemaker-us-west-2-986221661979/yolov11dataset\n",
      "Labels:  ['Car', 'Airplane', 'Ferry', 'Helicopter', 'Moterbike']\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset S3 location: \", dataset_s3_uri)\n",
    "print(\"Labels: \", labels)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
